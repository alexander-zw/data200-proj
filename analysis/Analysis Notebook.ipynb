{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import model_selection\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling and Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains product reviews and metadata from Amazon, including reviews spanning May 1996 - July 2014.\n",
    "This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load the data into dataframes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url,filename)\n",
    "        \n",
    "    # Load the data.\n",
    "    data = []\n",
    "    with gzip.open(filename) as f:\n",
    "        for l in f:\n",
    "            data.append(json.loads(l.strip()))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    print('data shape:', df.shape)\n",
    "    print('first rows of data:')\n",
    "    display(df.head(3))\n",
    "    return df\n",
    "\n",
    "def get_metadata_with_ratings(reviews, metadata):\n",
    "    ratings = reviews[['asin', 'overall']].groupby('asin').agg('mean').rename(columns={'overall': 'rating'})\n",
    "    metadata_with_ratings = metadata.merge(ratings, how=\"left\", on=\"asin\")\n",
    "    \n",
    "    # Check how many products have ratings.\n",
    "    print('distribution of ratings:')\n",
    "    display(metadata_with_ratings['rating'].describe())\n",
    "    print('number of missing ratings:', metadata_with_ratings['rating'].isnull().sum())\n",
    "    return metadata_with_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_All_Beauty.json.gz'\n",
    "filename = \"../data/meta_All_Beauty.json.gz\"\n",
    "metadata = load_data(url,filename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/All_Beauty.json.gz'\n",
    "filename = \"../data/All_Beauty.json.gz\"\n",
    "reviews = load_data(url,filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging `Reviews` and `Metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_with_ratings = get_metadata_with_ratings(reviews, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `reviews` attributes:\n",
    "- reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- reviewerName - name of the reviewer\n",
    "- vote - helpful votes of the review\n",
    "- style - a disctionary of the product metadata, e.g., \"Format\" is \"Hardcover\"\n",
    "- reviewText - text of the review\n",
    "- overall - rating of the product\n",
    "- summary - summary of the review\n",
    "- unixReviewTime - time of the review (unix time)\n",
    "- reviewTime - time of the review (raw)\n",
    "- image - images that users post after they have received the product\n",
    "#### `metadata` attributes:\n",
    "- title - name of the product\n",
    "- feature - bullet-point format features of the product\n",
    "- description - description of the product\n",
    "- price - price in US dollars (at time of crawl)\n",
    "- image - url of the product image\n",
    "- related - related products (also bought, also viewed, bought together, buy after viewing)\n",
    "- salesRank - sales rank information\n",
    "- brand - brand name\n",
    "- categories - list of categories the product belongs to\n",
    "- tech1 - the first technical detail table of the product\n",
    "- tech2 - the second technical detail table of the product\n",
    "- similar - similar product table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews dataset is only used for EDA, not modeling purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granularity: \n",
    "The granularity of `metadata` is a single product and the granularity for `reviews` dataset is a single review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cleaning procedure is recommended by the documentation\n",
    "metadata = metadata.fillna('')\n",
    "unform = metadata[metadata.title.str.contains('getTime')] # unformatted rows\n",
    "print(len(metadata))\n",
    "print(len(unform))\n",
    "# Turns out there are no unformatted rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_feat(data, feat, as_int=False):\n",
    "    description = data[feat].describe()\n",
    "    if as_int:\n",
    "        description = description.astype(int)\n",
    "    display(description)\n",
    "    print(f'number of missing {feat}s:', data[feat].isnull().sum())\n",
    "\n",
    "def clean_features(metadata_with_ratings, training_data=None, output=True):\n",
    "    if training_data is None:\n",
    "        training_data = metadata_with_ratings\n",
    "    # Clean price.\n",
    "    metadata_with_ratings['price_float'] = pd.to_numeric(\n",
    "            metadata_with_ratings['price'].str.replace('$', ''), errors='coerce')\n",
    "    if output:\n",
    "        describe_feat(metadata_with_ratings, 'price_float')    \n",
    "    \n",
    "    # Clean sales rank.\n",
    "    metadata_with_ratings['rank_float'] = pd.to_numeric(metadata_with_ratings['rank'].str.replace(',', '') \\\n",
    "                                                      .str.extract('^(\\d+)', expand=False), errors='coerce')\n",
    "    if output:\n",
    "        describe_feat(metadata_with_ratings, 'rank_float', as_int=True)\n",
    "    \n",
    "    # Add sales rank category.\n",
    "    metadata_with_ratings['rank_category'] = metadata_with_ratings['rank'] \\\n",
    "                .str.extract(' in (.+) \\(', expand=False) \\\n",
    "                .str.replace('&amp;', '&')\n",
    "    if output:\n",
    "        print('categories:')\n",
    "        print(metadata_with_ratings['rank_category'].value_counts())\n",
    "    \n",
    "    # Clean reviews\n",
    "    \n",
    "    \n",
    "    # Clean description.\n",
    "    metadata_with_ratings['description_str'] = metadata_with_ratings['description'].str.join('\\n')\n",
    "    \n",
    "    # Clean brand.\n",
    "    brand_counts = training_data['brand'].value_counts().iloc[1:]\n",
    "    metadata_with_ratings['brand_count'] = pd.to_numeric(metadata_with_ratings['brand'].replace(brand_counts),\n",
    "                                                         errors='coerce').fillna(0)\n",
    "    if output:\n",
    "        describe_feat(metadata_with_ratings, 'brand_count')\n",
    "    metadata_with_ratings['top_brand'] = (metadata_with_ratings['brand_count'] > 20).astype(int)\n",
    "    if output:\n",
    "        print('percentage top brand:', metadata_with_ratings['top_brand'].mean())\n",
    "        \n",
    "def transform_col(data, func, feat, new_feat):\n",
    "    data[new_feat] = func(metadata_with_ratings[feat])\n",
    "    data[new_feat].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_features(metadata_with_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning operations performed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `price`: Extracted the price value as a float into `price_float`\n",
    "- `sales rank`: \n",
    "    - Extracted the rank value as a float into `rank_float`\n",
    "    - Created a new attribute called `rank_category`\n",
    "- `description`: Removed \\n from the descriptions\n",
    "- `Brand`: Added `brand_counts`, the number of products sold by that brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, products with missing price do exist. We now decide on whether we remove these products from the review dataset or we keep them and try coming up with a approximation for their price.\n",
    "Let's check what proportion of the product dataset is missing a price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"About \",metadata_with_ratings[np.isnan(metadata_with_ratings['price_float'])].shape[0]/metadata_with_ratings.shape[0] * 100,\"% products are missing a price value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proportion is very huge. Let's see if we can identify a pattern in the product metadata for missing prices during EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_col(metadata_with_ratings, np.log, 'rank_float', 'log_rank')\n",
    "transform_col(metadata_with_ratings, np.sqrt, 'rank_float', 'sqrt_rank')\n",
    "transform_col(metadata_with_ratings, lambda x: x * x, 'price_float', 'sq_price')\n",
    "transform_col(metadata_with_ratings, lambda x: x * x, 'brand_count', 'sq_brand_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering operations performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove products with incorrect rank\n",
    "metadata_beauty = metadata_with_ratings.query('rank_category == \"Beauty & Personal Care\"')\n",
    "metadata_beauty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove products with out price, which likely means the product is no longer available.\n",
    "metadata_available = metadata_beauty[~np.isnan(metadata_beauty['price_float'])]\n",
    "metadata_available.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_joint_reg(data, x, y, title='', plot_figure=True):\n",
    "    if plot_figure:\n",
    "        sns.jointplot(data=data, x=x, y=y, kind='reg',\n",
    "                      scatter_kws={'alpha': 0.1, 's': 15}, line_kws={'color': 'r'}) \\\n",
    "                .fig.suptitle(title)\n",
    "    data_cleaned = data.query(f'not {x}.isnull() and not {y}.isnull()')\n",
    "    X = data_cleaned[x].to_numpy()[:,None]\n",
    "    model = LinearRegression().fit(X, data_cleaned[y])\n",
    "    print(f'y = {model.intercept_} + {model.coef_[0]} * x, r^2 = {model.score(X, data_cleaned[y])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Missing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we try and address the missing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noprice = metadata[metadata['price'] =='']\n",
    "withprice = metadata[metadata['price'] !='']\n",
    "brands_no_price = noprice['brand'].value_counts(dropna=False).to_frame().reset_index()\n",
    "brands_no_price.columns = ['brand','no_price']\n",
    "brands_with_price = withprice['brand'].value_counts(dropna=False).to_frame().reset_index()\n",
    "brands_with_price.columns = ['brand','with_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if not having prices is a brand specific pattern or product specific. If this is not specific to a brand, we can use the average price for all the products of a brand to assign the missing price. Unfortuately, if there is a brand with no products priced, we will have to discard that data as the proportion of products with missing price data is already very big and making assumptions will skew the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`price_exist` is a dataframe with `brand` name,`total` # products, # products with price (`with_price`), # products without price(`no_price`) and a boolean feature which is `True` if the brand has both priced and unpriced products otherwies `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_exist = metadata['brand'].value_counts(dropna=False).to_frame().reset_index()\n",
    "price_exist.columns = ['brand','total']\n",
    "price_exist.head()\n",
    "\n",
    "price_exist = price_exist.merge(brands_with_price,how = 'left',on = 'brand').merge(brands_no_price,how = 'left',on = 'brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_exist.head()\n",
    "price_exist.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[metadata['brand'] == 'Gillette'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata.query(\"brand== 'Gillette'\").loc[57,'description'],\"\\n\")\n",
    "print(metadata.query(\"brand== 'Gillette'\").loc[57,'title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above product is no longer sold by Gillette on Amazon. Most of the products that are missing prices are the ones that are no longer available on Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(price_exist['total'],price_exist['no_price']/price_exist['total'],)\n",
    "plt.xlabel(\"Total Products Sold\")\n",
    "plt.ylabel(\"Proportion of Products without Price\")\n",
    "plt.title(\"Proportion of Products without price by Brand\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extreme right point is the one with missing brand name. It probably represents unbranded products. We can disregard those products for now. And to reduce overplotting, we can change the limits of x axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(price_exist['total'],price_exist['no_price']/price_exist['total'],)\n",
    "plt.xlim(0,100)\n",
    "plt.xlabel(\"Total Products Sold\")\n",
    "plt.ylabel(\"Proportion of Products without Price\")\n",
    "plt.title(\"Proportion of Products without price by Brand\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this plot includes a lot of brands which only ever sold a few products and then shut down. Let's just look at the active brands. These are the brands which have atleast a few products with prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = price_exist['with_price']>5\n",
    "x = price_exist.loc[filtered,'total']\n",
    "y = price_exist.loc[filtered,'no_price']/price_exist.loc[filtered,'total']\n",
    "plt.scatter(x,y)\n",
    "plt.xlim(0,100)\n",
    "plt.xlabel(\"Total Products Sold\")\n",
    "plt.ylabel(\"Proportion of Products without Price\")\n",
    "plt.title(\"Proportion of Products without price by active Brands\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a positive correlation between the total number of products sold by active brands and the proportion of product without prices. This tells us that larger the brand the quicker they upgrade their inventory on Amazon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extracting the sentiment of the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed with extracting the sentiment from the reviews. We will then compare it with the rating and see what correlations we expect. One should expect a positive correlation. We will then compare sentiment against the prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall be using VADER. The cell below loads the data containing all sentiments into a dataframe called `sent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_file = \"../data/vader_lexicon.txt\"\n",
    "print(''.join(open(vader_file).readlines()[:10]))\n",
    "sent = pd.read_csv('data/vader_lexicon.txt',sep = '\\t',index_col=0,header=None, usecols = [0,1], names = ['token',\"polarity\"])\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again look at the `reviews` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the `reviewText`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead we need to first remove all the punctuation. Below is a regex which will capture all the punctuations within text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_re = r'[^\\w_\\s]'\n",
    "\n",
    "def sanitize_texts(df):\n",
    "    punct_re = r'[^\\w_\\s\\n]'    \n",
    "    df[\"clean_text\"] = df['reviewText'].str.lower().str.replace(punct_re,\" \",regex = True)\n",
    "    df[\"clean_summary\"] = df['summary'].str.lower().str.replace(punct_re,\" \",regex = True)\n",
    "    return df\n",
    "\n",
    "reviews = sanitize_texts(reviews)\n",
    "reviews[\"clean_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"unixReviewTime\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our review dataset doesn't have a primary key. Let's create one!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"primary_key\"] = reviews.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"primary_key\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up using the index as our primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tidy_format(df):\n",
    "    tidy = (\n",
    "        df[\"clean_text\"]\n",
    "        .str.split()\n",
    "        .explode()\n",
    "        .to_frame()\n",
    "        .rename(columns={\"clean_text\": \"word\"})\n",
    "    )\n",
    "    return tidy\n",
    "\n",
    "tidyReviewText = to_tidy_format(reviews)\n",
    "tidyReviewText.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in the Polarity Score:\n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each review: we can join the table with the lexicon table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polarity(df, tidy_df):\n",
    "    df[\"polarity\"] = (\n",
    "        tidy_df\n",
    "        .merge(sent, how='left', left_on='word', right_index=True)\n",
    "        .reset_index()\n",
    "        .loc[:, ['index', 'polarity']]\n",
    "        .groupby('index')\n",
    "        .sum()\n",
    "        .fillna(0)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "reviews = add_polarity(reviews, tidyReviewText)\n",
    "reviews.query(\"polarity<0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the polarity in place, let's see how polarity correlates with rating. We'll first do so without grouping by product and just focus on reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"overall\",y =\"polarity\",data = reviews)\n",
    "plt.ylim(-100,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as one would expect that the polarity of reviews would go up as the rating increases. Indeed, this is the case. Let's compare the correlation now for verified and non-verified users separately and we focus only on the IQR regions and not worry about outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"overall\",y =\"polarity\",data = reviews, hue = 'verified')\n",
    "plt.ylim(-25,25)\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Polarity\")\n",
    "plt.title(\"Polarity of reviews vs Rating\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verified users have a narrower IQR. This is because number of verified users is way more than the number of unverified users. Hence, more variability is encountered in unverified users response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['verified'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Sales Rank vs Avg Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure what we want to use as the training label, we narrowed down two candidates: Amazon product category sales rank, and the average review rating. To do this, we will explore the relationship of these two variables in the following three plots. Because of the peculiar structure of sales rank, which is heavily right-skewed, we tried using the log, square root, and raw values of sales rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='log_rank', y='rating', title='Log sales rank vs rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='sqrt_rank', y='rating', title='Square root sales rank vs rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='rank_float', y='rating', title='Sales rank vs rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sales rank is distributed most evenly when a square root transformation is applied, so we will be using the square root sales rank from now on. Regardless, there is a negative correlation between sales rank and rating. This makes sense, because products with higher ratings are likely to sell better. However, the correlation is not very strong: only about 1% of variance in the rating is explained by sales rank. This shows that neither value alone is enough to infer the other, so we must decide which variable is more important for us to predict.\n",
    "\n",
    "We decided to predict the square root sales rank, because any seller is likely to be more concerned about sales revenue. In addition, it has a wider range of values, whereas the rating only ranges from 1 to 5, so it might cause trouble for linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price vs Sales Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore the relationship between the price and square root sales rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_truncated = metadata_available.query('price_float < 100')\n",
    "plot_joint_reg(data=price_truncated, x='price_float', y='sqrt_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there is a positive correlation between the two, which means more expensive products tend to sell worse. The correlation is very weak, but the residuals do not appear to form any pattern, meaning we do not need to further explore a more complicated relationship. This makes sense, because many factors other than price affect the sales performance. This preliminary analysis suggests that the price will be a useful feature to predict the sales rank, though we cannot jump to any conclusions before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now explore the effect of the product brand on the sales rank. First, we graph the frequency certain brands appear in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_counts = metadata_available['brand'].value_counts().iloc[1:] # Remove blank ''.\n",
    "display(brand_counts[:10])\n",
    "\n",
    "brand_counts_filtered = brand_counts[brand_counts > 10]\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.arange(len(brand_counts_filtered)), brand_counts_filtered)\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('brand index')\n",
    "plt.title('Frequency of occurence for brands, from most to least common')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot the relationship between the number of times a product's brand appears vs the square root rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='brand_count', y='sqrt_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also explore whether a brand appearing over 20 times in the data makes it sell better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=metadata_available, x='top_brand', y='sqrt_rank');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, it appears that a few brands appear very frequently, while many other brands only occur in a few products. In addition, frequent brands seem to actually sell worse on average than infrequent brands, though with an extremely weak correlation. The brand count and top brand indicator are likely useful for prediction, but the one hot encoding for brand is likely also needed to take full advantage of the brand information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there any correlation between the variables you are interested in exploring?\n",
    "Correlations explored:\n",
    "1. Proportion of products by brand in the dataset without prices vs total products sold by that brand:\n",
    "   We observed that there was poitive correlation. Which meand uf a brand sold a larger number of products, it will have more products without prices. This shows that larger brands have to keep upgrading their inventory on Amazon to thrive.\n",
    "2. Polarity of ReviewText vs Rating: Positive correlation\n",
    "3. Square Root Sales Rank vs Average Rating: Negative correlation, but very weak.\n",
    "4. Price vs Square Root Sales Rank: Positive correlation, but weak.\n",
    "5. Brand Frequency vs Square Root Sals Rank: Positive correlation, very weak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling Utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(data, feat, max_words=100, encoder=None, output=True):\n",
    "    if not encoder:\n",
    "        encoder = CountVectorizer(max_features=max_words, stop_words='english')\n",
    "        encoder.fit(data[feat])\n",
    "    X = encoder.transform(data[feat]).toarray()\n",
    "\n",
    "    if output:\n",
    "        print('first 50 features:', encoder.get_feature_names()[:50])\n",
    "        print('feature matrix shape', X.shape)\n",
    "    return X, encoder\n",
    "\n",
    "def onehot_encode(data, feat, max_categories=100, encoder=None):\n",
    "    cat_counts = data[feat].value_counts()\n",
    "    categories = cat_counts[1:max_categories + 1].index.tolist()\n",
    "    raw_features = data[feat].to_numpy()[:,None]\n",
    "    if not encoder:\n",
    "        encoder = OneHotEncoder(categories=[categories], handle_unknown='ignore', sparse=False)\n",
    "        encoder.fit(raw_features)\n",
    "    return encoder.transform(raw_features), encoder\n",
    "\n",
    "def fill_missing(data, feat, imputer=None):\n",
    "    if not imputer:\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "def standardize(X, scale_columns, encoder=None, output=True):\n",
    "    X_to_scale = X[:,scale_columns]\n",
    "    if not encoder:\n",
    "        encoder = StandardScaler()\n",
    "        encoder.fit(X_to_scale)\n",
    "    X[:,scale_columns] = encoder.transform(X_to_scale)\n",
    "    if output:\n",
    "        print('mean:', encoder.mean_)\n",
    "        print('standard deviation:', encoder.scale_)\n",
    "    return X, encoder\n",
    "\n",
    "numerical_features = ['price_float', 'sq_price', 'brand_count', 'sq_brand_count', 'top_brand']\n",
    "standardize_cols = [0, 1, 2, 3] # Column indexes that should be standardized.\n",
    "\n",
    "def get_feat_matrix(data, encoders=None):\n",
    "    if not encoders:\n",
    "        encoders = [None, None, None, None]\n",
    "    X1 = data[numerical_features].to_numpy()\n",
    "    X2, encoders[0] = onehot_encode(data, 'brand', encoder=encoders[0])\n",
    "    X3, encoders[1] = extract_words(data, 'title', encoder=encoders[1], output=False)\n",
    "    X4, encoders[2] = extract_words(data, 'description_str', 500, encoder=encoders[2], output=False)\n",
    "    X = np.hstack([X1, X2, X3, X4])\n",
    "    X, encoders[3] = standardize(X, standardize_cols, encoder=encoders[3], output=False)\n",
    "    return X, encoders\n",
    "\n",
    "def get_feature_names(encoders):\n",
    "    features = np.append(numerical_features, encoders[0].get_feature_names())\n",
    "    features = np.append(features, np.char.add('title_', encoders[1].get_feature_names()))\n",
    "    features = np.append(features, np.char.add('descr_', encoders[2].get_feature_names()))\n",
    "    return features\n",
    "\n",
    "def train_test_split(data, label):\n",
    "    data_train, data_test, y_train, y_test = \\\n",
    "            model_selection.train_test_split(data, data[label], test_size=0.2, random_state=314)\n",
    "    clean_features(data_train, training_data=data_train, output=False)\n",
    "    clean_features(data_test, training_data=data_train, output=False)\n",
    "    X_train, encoders = get_feat_matrix(data_train)\n",
    "    X_test, _ = get_feat_matrix(data_test, encoders=encoders)\n",
    "    print(f'Training data of shape {X_train.shape}, test data {X_test.shape}')\n",
    "    return X_train, X_test, y_train, y_test, encoders\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def mean_proportional_error(y, y_pred):\n",
    "    return np.sqrt(np.mean(((y - y_pred) / y)**2))\n",
    "\n",
    "def train_ridge(X_train, X_test, y_train, y_test):\n",
    "    alpha_exponents = np.arange(-5, 5, 0.2) * np.log(10)\n",
    "    model = RidgeCV(alphas=np.exp(alpha_exponents))\n",
    "    return train_model(X_train, X_test, y_train, y_test, model)\n",
    "    \n",
    "def train_model(X_train, X_test, y_train, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    train_r2 = model.score(X_train, y_train)\n",
    "    test_r2 = model.score(X_test, y_test)\n",
    "    train_loss = rmse(y_train, y_train_pred)\n",
    "    test_loss = rmse(y_test, y_test_pred)\n",
    "    train_err = mean_proportional_error(y_train, y_train_pred)\n",
    "    test_err = mean_proportional_error(y_test, y_test_pred)\n",
    "    \n",
    "    print('training r^2:', train_r2)\n",
    "    print('test r^2:', test_r2)\n",
    "    print('training loss:', train_loss)\n",
    "    print('test loss:', test_loss)\n",
    "    print('training proportional loss:', train_err)\n",
    "    print('test proportional loss:', test_err)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test data split, 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'sqrt_rank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, encoders = train_test_split(metadata_available, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ridge regression, we use `sklearn`'s built-in cross validation to tune the hyperparameter. After training is complete, we checked for the top features used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = train_ridge(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "feats = get_feature_names(encoders)\n",
    "top_coefs = np.argsort(-np.abs(model_ridge.coef_))[:n]\n",
    "print('top features for Ridge model:', feats[top_coefs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some manual validation on the test $r^2$, it was determined that $C=250$ was the best hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May take several minutes to train.\n",
    "model_svm = train_model(X_train, X_test, y_train, y_test, SVR(C=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some manual validation on the test $r^2$, it was determined that `max_depth=27`\n",
    "(along with default values for other variables such as `min_leaf_size`) was the best hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting n_estimators to be greater increases r^2 slightly but takes much longer, so I just went with\n",
    "# the default. Set n_jobs to make it faster.\n",
    "model_rf = train_model(X_train, X_test, y_train, y_test, RandomForestRegressor(max_depth=27, n_jobs=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a reference for the loss, we calculated the standard deviation (as well as other statistics) of the label.\n",
    "\n",
    "Change the `model` variable below to analyze a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_ridge\n",
    "\n",
    "describe_feat(metadata_available, label, as_int=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observed the first few predictions on the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "for i in range(10):\n",
    "    print(f'{y_train.iloc[i]:7.0f}, {y_train_pred[i]:7.0f}; {metadata_available.iloc[i][\"title\"]}')\n",
    "print()\n",
    "for i in range(10):\n",
    "    print(f'{y_test.iloc[i]:7.0f}, {y_test_pred[i]:7.0f}; '\n",
    "          f'{metadata_available.iloc[X_train.shape[0] + i][\"title\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plotted the proportional residuals for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y_train_pred - y_train) / np.abs(y_train)\n",
    "sns.histplot(residuals[residuals < 6])\n",
    "plt.xlabel('proprotional residuals of sqrt rank')\n",
    "plt.title('Distribution of porportional residuals of Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y_test_pred - y_test) / np.abs(y_test)\n",
    "sns.histplot(residuals[residuals < 6])\n",
    "plt.xlabel('proprotional residuals of sqrt rank')\n",
    "plt.title('Distribution of porportional residuals of Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few components your notebook must address:\n",
    "- What type of machine learning problem are you investigating?\n",
    "    - This is a supervised machine learning problem, with a regression model.\n",
    "- What model do you plan on using and why?\n",
    "    - We tried using ridge regression, soft-margin SVM, and vanilla random forest. Ridge regression was the most basic model, SVM because we believed nonlinear kernels could help reduce model bias, and random forest because it could combine features in nonlinear fashions even better.\n",
    "- Does your model require hyperparameter tuning? If so, how do you approach it?\n",
    "    - We used built-in cross-validation with ridge regression, but because SVMs and random forest take much longer to train, we used manual validation on test $r^2$. Ridge and SVM each had one hyperparameter that controls the degree of regularization. Random forests used three hyperparameters, number of estimators, maximum tree depth, and minimum leaf size.\n",
    "- How do you engineer the features for your model? What are the rationales behind selecting these features?\n",
    "    - We selected only features that a product would reasonably have before launch. The price, brand, title, and description were used because they are easy to incorporate (as opposed to the image, for example), and would likely influence the sales performance. The top 100 brands were one-hot encoded. In order to assist linear models in using these features and allow nonlinear functions to be learned, number of times each brand occurred in the test set was added as a feature, as was a binary feature that described whether the brand appeared over 20 times. The price and brand count squared were also added. The 100 most common words in the title and 500 most common words in the describtion were one-hot encoded.\n",
    "- How do you perform cross validation on your model?\n",
    "    - We used the `sklearn` built-in cross validation for ridge regression, and manual validation for other models. In manual validation, the test data $r^2$ was used as a metric.\n",
    "- What loss metrics are you using to evaluate your model?\n",
    "    - The primary metric used was the correlation coefficient $r^2$, but the root mean square loss and root mean square proportional loss were used (see writeup for formulas).\n",
    "- From a bias-variance tradeoff standpoint, how do you assess the performance of your model? How do you check if it is overfitting?\n",
    "    - All three models were overfitting since they had higher training $r^2$ than test $r^2$, but the variance was not so high as to make predictions useless. The random forest in particular had the highest tendency to overfit. We aimed for highest test $r^2$ to prevent the models from overfitting too much.\n",
    "    - All three models also had relatively high bias, since even their training $r^2$ was not very high, but this was to be expected and is justified in the writeup.\n",
    "- How would you improve your model based on the outcome?\n",
    "    - We might need to add more features (perhaps taking more brands and title/description words) and add more nonlinearity (other functional transformations on the features). We could also look into the unused features that are harder to incorporate, such as image and technical description. We might also explore using a deep neural network for maximum expressiveness."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
