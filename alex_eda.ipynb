{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Dataset documentation](https://nijianmo.github.io/amazon/index.html)\n",
    "- [Complete Metadata files](http://deepyeti.ucsd.edu/jianmo/amazon/index.html)\n",
    "- [Pandas reference sheet](https://ds100.org/sp21/resources/assets/exams/sp20/sp20_checkpoint_reference_sheet.pdf)\n",
    "- [Data-200 Google Doc](https://docs.google.com/document/d/19HWODy5kpWoUB7BEKEmKLbRnK8MC1fBmRat_WP7vfNc/edit)\n",
    "- [Grad Project Guidelines](https://ds100.org/sp21/grad_proj/gradproject/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import model_selection\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils.\n",
    "\n",
    "#################################### Loading data. ####################################\n",
    "\n",
    "def load_data(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url,filename)\n",
    "        \n",
    "    # Load the data.\n",
    "    data = []\n",
    "    with gzip.open(filename) as f:\n",
    "        for l in f:\n",
    "            data.append(json.loads(l.strip()))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    print('data shape:', df.shape)\n",
    "    print('first rows of data:')\n",
    "    display(df.head(3))\n",
    "    return df\n",
    "\n",
    "def get_metadata_with_ratings(reviews, metadata):\n",
    "    ratings = reviews[['asin', 'overall']].groupby('asin').agg('mean').rename(columns={'overall': 'rating'})\n",
    "    metadata_with_ratings = metadata.merge(ratings, how=\"left\", on=\"asin\")\n",
    "    \n",
    "    # Check how many products have ratings.\n",
    "    print('distribution of ratings:')\n",
    "    display(metadata_with_ratings['rating'].describe())\n",
    "    print('number of missing ratings:', metadata_with_ratings['rating'].isnull().sum())\n",
    "    return metadata_with_ratings\n",
    "\n",
    "################################## Data exploration. ##################################\n",
    "\n",
    "def describe_feat(data, feat, as_int=False):\n",
    "    description = data[feat].describe()\n",
    "    if as_int:\n",
    "        description = description.astype(int)\n",
    "    display(description)\n",
    "    print(f'number of missing {feat}s:', data[feat].isnull().sum())\n",
    "\n",
    "def plot_joint_reg(data, x, y, title='', plot_figure=True):\n",
    "    if plot_figure:\n",
    "        sns.jointplot(data=data, x=x, y=y, kind='reg',\n",
    "                      scatter_kws={'alpha': 0.1, 's': 15}, line_kws={'color': 'r'}) \\\n",
    "                .fig.suptitle(title)\n",
    "    data_cleaned = data.query(f'not {x}.isnull() and not {y}.isnull()')\n",
    "    X = data_cleaned[x].to_numpy()[:,None]\n",
    "    model = LinearRegression().fit(X, data_cleaned[y])\n",
    "    print(f'y = {model.intercept_} + {model.coef_[0]} * x, r^2 = {model.score(X, data_cleaned[y])}')\n",
    "\n",
    "def plot_pca_variance_ratios(X, n_components=10):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    ratios = pca.explained_variance_ratio_\n",
    "    x = np.arange(len(ratios))\n",
    "    plt.plot(x, ratios)\n",
    "    plt.xlabel('component index')\n",
    "    plt.ylabel('explained variance ratio')\n",
    "    plt.title(f'First {n_components} PCA components')\n",
    "\n",
    "###################################### Modeling. ######################################\n",
    "\n",
    "def clean_features(metadata_with_ratings, training_data=None, output=True):\n",
    "    if training_data is None:\n",
    "        training_data = metadata_with_ratings\n",
    "    # Clean price.\n",
    "    metadata_with_ratings['price_float'] = pd.to_numeric(\n",
    "            metadata_with_ratings['price'].str.replace('$', ''), errors='coerce')\n",
    "    if output:\n",
    "        describe_feat(metadata_with_ratings, 'price_float')\n",
    "    \n",
    "    # Clean sales rank.\n",
    "    metadata_with_ratings['rank_float'] = pd.to_numeric(metadata_with_ratings['rank'].str.replace(',', '') \\\n",
    "                                                      .str.extract('^(\\d+)', expand=False), errors='coerce')\n",
    "    if output:\n",
    "        describe_feat(metadata_with_ratings, 'rank_float', as_int=True)\n",
    "    \n",
    "    # Add sales rank category.\n",
    "    metadata_with_ratings['rank_category'] = metadata_with_ratings['rank'] \\\n",
    "                .str.extract(' in (.+) \\(', expand=False) \\\n",
    "                .str.replace('&amp;', '&')\n",
    "    if output:\n",
    "        print('categories:')\n",
    "        print(metadata_with_ratings['rank_category'].value_counts())\n",
    "    \n",
    "    # Clean description.\n",
    "    metadata_with_ratings['description_str'] = metadata_with_ratings['description'].str.join('\\n')\n",
    "    \n",
    "    # Clean brand.\n",
    "    brand_counts = training_data['brand'].value_counts().iloc[1:]\n",
    "    metadata_with_ratings['brand_count'] = pd.to_numeric(metadata_with_ratings['brand'].replace(brand_counts),\n",
    "                                                         errors='coerce').fillna(0)\n",
    "    if output:\n",
    "        describe_feat(metadata_with_ratings, 'brand_count')\n",
    "    metadata_with_ratings['top_brand'] = (metadata_with_ratings['brand_count'] > 20).astype(int)\n",
    "    if output:\n",
    "        print('percentage top brand:', metadata_with_ratings['top_brand'].mean())\n",
    "    \n",
    "def transform_col(data, func, feat, new_feat):\n",
    "    data[new_feat] = func(metadata_with_ratings[feat])\n",
    "    data[new_feat].describe()\n",
    "\n",
    "def extract_words(data, feat, max_words=100, encoder=None, output=True):\n",
    "    if not encoder:\n",
    "        encoder = CountVectorizer(max_features=max_words, stop_words='english')\n",
    "        encoder.fit(data[feat])\n",
    "    X = encoder.transform(data[feat]).toarray()\n",
    "\n",
    "    if output:\n",
    "        print('first 50 features:', encoder.get_feature_names()[:50])\n",
    "        print('feature matrix shape', X.shape)\n",
    "    return X, encoder\n",
    "\n",
    "def onehot_encode(data, feat, max_categories=100, encoder=None):\n",
    "    cat_counts = data[feat].value_counts()\n",
    "    categories = cat_counts[1:max_categories + 1].index.tolist()\n",
    "    raw_features = data[feat].to_numpy()[:,None]\n",
    "    if not encoder:\n",
    "        encoder = OneHotEncoder(categories=[categories], handle_unknown='ignore', sparse=False)\n",
    "        encoder.fit(raw_features)\n",
    "    return encoder.transform(raw_features), encoder\n",
    "\n",
    "def fill_missing(data, feat, imputer=None):\n",
    "    if not imputer:\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "def standardize(X, scale_columns, encoder=None, output=True):\n",
    "    X_to_scale = X[:,scale_columns]\n",
    "    if not encoder:\n",
    "        encoder = StandardScaler()\n",
    "        encoder.fit(X_to_scale)\n",
    "    X[:,scale_columns] = encoder.transform(X_to_scale)\n",
    "    if output:\n",
    "        print('mean:', encoder.mean_)\n",
    "        print('standard deviation:', encoder.scale_)\n",
    "    return X, encoder\n",
    "\n",
    "def get_feat_matrix(data, encoders=None):\n",
    "    if not encoders:\n",
    "        encoders = [None, None, None, None]\n",
    "    X1 = data[['price_float', 'brand_count', 'top_brand']].to_numpy()\n",
    "    X2, encoders[0] = onehot_encode(data, 'brand', encoder=encoders[0])\n",
    "    X3, encoders[1] = extract_words(data, 'title', encoder=encoders[1], output=False)\n",
    "    X4, encoders[2] = extract_words(data, 'description_str', 500, encoder=encoders[2], output=False)\n",
    "    X = np.hstack([X1, X2, X3, X4])\n",
    "    X, encoders[3] = standardize(X, [0, 1], encoder=encoders[3], output=False)\n",
    "    return X, encoders\n",
    "\n",
    "def train_test_split(data, label):\n",
    "    data_train, data_test, y_train, y_test = \\\n",
    "            model_selection.train_test_split(data, data[label], test_size=0.2, random_state=314)\n",
    "    clean_features(data_train, training_data=data_train, output=False)\n",
    "    clean_features(data_test, training_data=data_train, output=False)\n",
    "    X_train, encoders = get_feat_matrix(data_train)\n",
    "    X_test, _ = get_feat_matrix(data_test, encoders=encoders)\n",
    "    print(f'Training data of shape {X_train.shape}, test data {X_test.shape}')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def mean_proportional_error(y, y_pred):\n",
    "    return np.sqrt(np.mean(((y - y_pred) / y)**2))\n",
    "\n",
    "def train_ridge(X_train, X_test, y_train, y_test):\n",
    "    alpha_exponents = np.arange(-5, 5, 0.2) * np.log(10)\n",
    "    model = RidgeCV(alphas=np.exp(alpha_exponents))\n",
    "    return train_model(X_train, X_test, y_train, y_test, model)\n",
    "    \n",
    "def train_model(X_train, X_test, y_train, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    train_r2 = model.score(X_train, y_train)\n",
    "    test_r2 = model.score(X_test, y_test)\n",
    "    train_loss = rmse(y_train, y_train_pred)\n",
    "    test_loss = rmse(y_test, y_test_pred)\n",
    "    train_err = mean_proportional_error(y_train, y_train_pred)\n",
    "    test_err = mean_proportional_error(y_test, y_test_pred)\n",
    "    \n",
    "    print('training r^2:', train_r2)\n",
    "    print('test r^2:', test_r2)\n",
    "    print('training loss:', train_loss)\n",
    "    print('test loss:', test_loss)\n",
    "    print('training proportional loss:', train_err)\n",
    "    print('test proportional loss:', test_err)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata:\n",
    "\n",
    "- asin - ID of the product, e.g. 0000031852\n",
    "- title - name of the product\n",
    "- feature - bullet-point format features of the product\n",
    "- description - description of the product\n",
    "- price - price in US dollars (at time of crawl)\n",
    "- image - url of the product image\n",
    "- related - related products (also bought, also viewed, bought together, buy after viewing)\n",
    "- salesRank - sales rank information\n",
    "- brand - brand name\n",
    "- categories - list of categories the product belongs to\n",
    "- tech1 - the first technical detail table of the product\n",
    "- tech2 - the second technical detail table of the product\n",
    "- similar - similar product table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews:\n",
    "\n",
    "- reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- reviewerName - name of the reviewer\n",
    "- vote - helpful votes of the review\n",
    "- style - a disctionary of the product metadata, e.g., \"Format\" is \"Hardcover\"\n",
    "- reviewText - text of the review\n",
    "- overall - rating of the product\n",
    "- summary - summary of the review\n",
    "- unixReviewTime - time of the review (unix time)\n",
    "- reviewTime - time of the review (raw)\n",
    "- image - images that users post after they have received the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/All_Beauty.json.gz\"\n",
    "filename = 'data/All_Beauty.json.gz'\n",
    "reviews = load_data(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_All_Beauty.json.gz\"\n",
    "filename = 'data/Meta_All_Beauty.json.gz'\n",
    "metadata = load_data(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge reviews and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_with_ratings = get_metadata_with_ratings(reviews, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_features(metadata_with_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_col(metadata_with_ratings, np.log, 'rank_float', 'log_rank')\n",
    "transform_col(metadata_with_ratings, np.sqrt, 'rank_float', 'sqrt_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove products with incorrect rank\n",
    "metadata_beauty = metadata_with_ratings.query('rank_category == \"Beauty & Personal Care\"')\n",
    "metadata_beauty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove products with out price, which likely means the product is no longer available.\n",
    "metadata_available = metadata_beauty.query('not price_float.isnull()')\n",
    "metadata_available.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales rank vs average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two products with no reviews.\n",
    "metadata_beauty.query('rating.isnull()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three plots show the correlation between sales rank and average rating. The correlation is slightly different for transformed versions of the rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='log_rank', y='rating', title='Log sales rank vs rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='sqrt_rank', y='rating', title='Square root sales rank vs rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='rank_float', y='rating', title='Sales rank vs rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price vs sales rank and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_truncated = metadata_available.query('price_float < 100')\n",
    "plot_joint_reg(data=price_truncated, x='price_float', y='sqrt_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=price_truncated, x='price_float', y='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_counts = metadata_available['brand'].value_counts().iloc[1:] # Remove blank ''.\n",
    "display(brand_counts[:10])\n",
    "display(brand_counts.describe())\n",
    "\n",
    "brand_counts_filtered = brand_counts_filtered[brand_counts_filtered > 10]\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.arange(len(brand_counts_filtered)), brand_counts_filtered)\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Frequency of occurence for brands, from most to least common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='brand_count', y='sqrt_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=metadata_available, x='brand_count', y='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=metadata_available, x='top_brand', y='sqrt_rank');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=metadata_available, x='top_brand', y='rating');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_words(metadata_available, 'title');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_words(metadata_available, 'description_str', 500);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, encoders = get_feat_matrix(metadata_available)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_variance_ratios(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X)\n",
    "components_df = pd.DataFrame(data=components, columns=['component1', 'component2'])\n",
    "data_with_pca = pd.concat([metadata_available.reset_index(drop=True),\n",
    "                           components_df.reset_index(drop=True)], axis=1)\n",
    "data_with_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data_with_pca.query('component1 < -0.08 and component2 < -0.2')\n",
    "print('after filtering:', filtered_data.shape)\n",
    "sns.jointplot(data=filtered_data, x='component1', y='component2', kind='scatter', alpha=0.1) \\\n",
    "    .fig.suptitle('Distribution of first 2 PCA components (cropped)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=data_with_pca.sample(1000), x='component1', y='component2', kind='scatter', alpha=0.1) \\\n",
    "    .fig.suptitle('Distribution of first 2 PCA components (sampled)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=filtered_data, x='component1', y='rank_float',\n",
    "               title='First principal component vs sales rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=filtered_data, x='component1', y='rating', title='First principal component vs rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=filtered_data, x='component2', y='rank_float',\n",
    "               title='Second principal component vs sales rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint_reg(data=filtered_data, x='component2', y='rating', title='Second principal component vs rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('component 1 vs rank')\n",
    "plot_joint_reg(data=data_with_pca, x='component1', y='rank_float', plot_figure=False)\n",
    "print('component 1 vs rating')\n",
    "plot_joint_reg(data=data_with_pca, x='component1', y='rating', plot_figure=False)\n",
    "print('component 2 vs rank')\n",
    "plot_joint_reg(data=data_with_pca, x='component2', y='rank_float', plot_figure=False)\n",
    "print('component 2 vs rating')\n",
    "plot_joint_reg(data=data_with_pca, x='component2', y='rating', plot_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'sqrt_rank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(metadata_available, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_ridge(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(model.coef_ * 10).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train, X_test, y_train, y_test, LassoCV(tol=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May take several minutes to train.\n",
    "model = train_model(X_train, X_test, y_train, y_test, SVR(C=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting n_estimators to be greater increases r^2 slightly but takes much longer, so I just went with\n",
    "# the default. Set n_jobs to make it faster.\n",
    "model = train_model(X_train, X_test, y_train, y_test, RandomForestRegressor(max_depth=25, n_jobs=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train, X_test, y_train, y_test,\n",
    "                    GradientBoostingRegressor(n_estimators=200, max_depth=5, min_samples_leaf=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train, X_test, y_train, y_test, AdaBoostRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (common to all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_feat(metadata_available, label, as_int=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "for i in range(10):\n",
    "    print(f'{y_train.iloc[i]:7.0f}, {y_train_pred[i]:7.0f}')\n",
    "print()\n",
    "for i in range(10):\n",
    "    print(f'{y_test.iloc[i]:7.0f}, {y_test_pred[i]:7.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y_train_pred - y_train) / np.abs(y_train)\n",
    "sns.histplot(residuals[residuals < 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y_test_pred - y_test) / np.abs(y_test)\n",
    "sns.histplot(residuals[residuals < 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders[0].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
